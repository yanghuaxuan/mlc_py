{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1c0e449-5941-4be0-a25d-74a960fabc48",
   "metadata": {},
   "source": [
    "# MLC v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66398c96-368b-49db-9ad0-c5a3ea1f3334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf081ea-a338-406a-bf02-a4b03afe7b76",
   "metadata": {},
   "source": [
    "# The Equation for Backpropagation\n",
    "$$\\begin{array}{cll}\n",
    "\\delta^L & = & \\nabla_a C \\odot \\sigma'(z^L) \\\\ \n",
    "\\delta^l & = & ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l) \\\\\n",
    "\\frac{\\partial C}{\\partial b^l_j} & = & \\delta^l_j \\\\\n",
    "\\frac{\\partial C}{\\partial w^l_{jk}} & = & a_k^{l-1} \\delta^l_j\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3c9287-04cd-44c3-aa20-eaa429cf7ac6",
   "metadata": {},
   "source": [
    "# Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9908c1bd-9419-471f-9166-a22a323abdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        self.weights = np.random.rand(input_size, output_size)\n",
    "        self.bias = np.random.rand(1, output_size)\n",
    "        self.activate, self.d_activate = activation\n",
    "    def forward(self, x):\n",
    "        self.inputs = x\n",
    "        self.outputs = self.activate(x @ self.weights + self.bias)\n",
    "        return self.outputs\n",
    "    # Returns the delta for this layer\n",
    "    def err(self, delta_p, w_p):\n",
    "        #print(f\"delta: {delta_p}\")\n",
    "        delta = (delta_p @ w_p.T) * self.d_activate(self.outputs)\n",
    "        #delta = (w_p @ delta_p.T) * self.d_activate(self.outputs)\n",
    "        return delta\n",
    "    # Perform backpropagation, returning the weights and bias gradients for this layer\n",
    "    def back(self, delta):\n",
    "        #print(f\"self.inputs: {self.inputs.T}\")\n",
    "        #print(f\"delta input: {delta}\")\n",
    "        print(f\"delta: {delta}\")\n",
    "        print(f\"self.inputs.T: {self.inputs.T}\")\n",
    "        \n",
    "        grad_b = delta\n",
    "        grad_w =  self.inputs.T @ delta \n",
    "\n",
    "        return (grad_w, grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3818e3-bfe3-49b0-9153-17a08f3822dc",
   "metadata": {},
   "source": [
    "# Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "666ba494-042e-4efb-8439-bb4f37e7b926",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, loss, d_loss):\n",
    "        self.layers = []\n",
    "        self.loss = loss\n",
    "        self.d_loss = d_loss\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    def fit(self, inputs, labels, epochs, learn_rate):\n",
    "        # epoch is deprecated; no use pls\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch}\")\n",
    "            epoch_loss = 0 \n",
    "            grad_w = []\n",
    "            grad_b = []\n",
    "            for x in enumerate(inputs):\n",
    "                output = x[1]\n",
    "                # forward propagate\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward(output)\n",
    "\n",
    "                epoch_loss += self.loss(output, labels[x[0]])\n",
    "                \n",
    "                lg_w = [] # gradients are stored in reverse of layers\n",
    "                lg_b = []\n",
    "                # backpropagate\n",
    "                #delta = self.d_loss(output, labels[x[0]]) *  self.layers[-1].d_activate(output)\n",
    "                delta = self.d_loss(output, labels[x[0]]) * self.layers[-1].d_activate(output) \n",
    "                #print(f\"initial delta: {delta}\")\n",
    "                #print(f\"# of layers: {len(self.layers)}\")\n",
    "                for i in range(len(self.layers)-1, -1, -1):\n",
    "                    #print(f\"======At layer {i}\")\n",
    "                    gw, gb = self.layers[i].back(delta)\n",
    "                    lg_w.append(gw)\n",
    "                    lg_b.append(gb)\n",
    "                    if i-1 >= 0:\n",
    "                        delta = self.layers[i-1].err(delta, layer.weights)\n",
    "                grad_w.append(lg_w)\n",
    "                grad_b.append(lg_b)\n",
    "\n",
    "            # Sum and average\n",
    "            #grad_w = np.add.reduce(grad_w) / len(inputs)\n",
    "            #grad_b = np.add.reduce(grad_b) / len(inputs)\n",
    "            #print(f\"grad_b: {grad_b}\")\n",
    "            grad_b_avg = []\n",
    "            for i in range(len(grad_b[0])):\n",
    "                b0 = grad_b[0][i]\n",
    "                for j in range(1, len(grad_b)):\n",
    "                    #print(f\"+=+= {grad_b[j][i]}\")\n",
    "                    b0 += grad_b[j][i]\n",
    "                grad_b_avg.append(b0 / len(grad_b))\n",
    "\n",
    "            #print(f\"grad_w: {grad_w}\")\n",
    "            grad_w_avg = []\n",
    "            for i in range(len(grad_w[0])):\n",
    "                w0 = grad_w[0][i]\n",
    "                for j in range(1, len(grad_w)):\n",
    "                    #print(f\"+=+= {grad_w[j][i]}\")\n",
    "                    w0 += grad_w[j][i]\n",
    "                #print(f\"number of grad_ws: {len(grad_w)}\")\n",
    "                grad_w_avg.append(w0 / len(grad_w))\n",
    "            #print(f\"grad_w_avg: {grad_w_avg}\")\n",
    "                \n",
    "            \n",
    "            #print(f\"grad_w: {grad_w}\")\n",
    "            #print(f\"grad_b: {grad_b}\")\n",
    "            for i in range(len(self.layers)-1, -1, -1):\n",
    "                layer = self.layers[i]\n",
    "                #print(f\"layer weights: {layer.weights}\")\n",
    "                #print(f\"grad_w_avg: {grad_w_avg}\")\n",
    "                layer.weights -= learn_rate * grad_w_avg[-i+1]\n",
    "                layer.bias -= learn_rate * grad_b_avg[-i+1]\n",
    "            \n",
    "            print(f\"loss: {epoch_loss / len(inputs)}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f917886-71cb-4f50-9bde-6953bd22c3d8",
   "metadata": {},
   "source": [
    "# Activator functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b22021f-6987-44b6-95ca-0a5b6341b0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "def d_sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    return np.exp(-x) / ((1 + np.exp(-x))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31cf54c-3876-40c1-a48a-d800cf4c41dc",
   "metadata": {},
   "source": [
    "# Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfe25675-024f-4ef0-8621-e2bbe722c3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(dy, y):\n",
    "    return np.mean(np.power(dy-y, 2));\n",
    "\n",
    "def mse_prime(dy, y):\n",
    "    return 2*(dy-y)/dy.size;\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x);\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1-np.tanh(x)**2;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3419cf0-8744-412f-865d-65a3be936222",
   "metadata": {},
   "source": [
    "# Batchtize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44a43f83-add1-430a-8d17-2855fcb37a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def batchtize(inputs, labels, batch_size):\n",
    "    if len(inputs) != len(labels):\n",
    "        raise\n",
    "    if len(inputs) % batch_size != 0:\n",
    "        raise\n",
    "        \n",
    "    c = 0\n",
    "    chosen_indexes = []\n",
    "    total_x = []\n",
    "    total_y = []\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    while c < len(x):\n",
    "        i = randint(0, len(inputs)-1)\n",
    "        while i in chosen_indexes:\n",
    "            i = randint(0, len(inputs)-1)\n",
    "        chosen_indexes += [i]\n",
    "        if c != 0 and c % batch_size == 0:\n",
    "            total_x += batch_x\n",
    "            total_y += batch_y\n",
    "            batch_x = []\n",
    "            batch_y = []\n",
    "        #print(f\"inserting input: {inputs[i]}\")\n",
    "        batch_x.append(inputs[i])\n",
    "        batch_y.append(labels[i])\n",
    "        c+= 1\n",
    "        #print(\"ran!\")\n",
    "\n",
    "    total_x += batch_x\n",
    "    total_y += batch_y\n",
    "    \n",
    "    return (total_x, total_y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47e8623-f09f-4cbb-abd2-d7861535e851",
   "metadata": {},
   "source": [
    "# Test Batchtize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8f63f720-8c78-413f-99fc-70591a24a933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[0, 1], [1, 1], [0, 0], [1, 0]], [[1], [0], [0], [1]])\n"
     ]
    }
   ],
   "source": [
    "x = [[1,0], [0,1], [1,1], [0,0]]\n",
    "y = [[1], [1], [0], [0]]\n",
    "\n",
    "print(batchtize(x, y, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fbd7f1-e5b8-41a7-87cc-4df2a17d6762",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3b1a0dbd-2147-4f49-9701-b175cc30de7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_b[i]: [array([[0, 0]]), array([[1, 0]]), array([[0, 1]]), array([[1, 1]])]\n",
      "Epoch 0\n",
      "delta: [[0.86532852]]\n",
      "self.inputs.T: [[0.23863582]\n",
      " [0.09947087]\n",
      " [0.72542194]]\n",
      "delta: [[0.39980036 0.34511047 0.48405496]]\n",
      "self.inputs.T: [0 0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x_b)):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_b[i]: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx_b\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m     \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 32\u001b[0m, in \u001b[0;36mNetwork.fit\u001b[0;34m(self, inputs, labels, epochs, learn_rate)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#print(f\"initial delta: {delta}\")\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#print(f\"# of layers: {len(self.layers)}\")\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m#print(f\"======At layer {i}\")\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     gw, gb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     lg_w\u001b[38;5;241m.\u001b[39mappend(gw)\n\u001b[1;32m     34\u001b[0m     lg_b\u001b[38;5;241m.\u001b[39mappend(gb)\n",
      "Cell \u001b[0;32mIn[12], line 24\u001b[0m, in \u001b[0;36mLayer.back\u001b[0;34m(self, delta)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself.inputs.T: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs\u001b[38;5;241m.\u001b[39mT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m grad_b \u001b[38;5;241m=\u001b[39m delta\n\u001b[0;32m---> 24\u001b[0m grad_w \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m \n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (grad_w, grad_b)\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 2)"
     ]
    }
   ],
   "source": [
    "net = Network(mse, mse_prime)\n",
    "activator = (tanh, tanh_prime)\n",
    "\n",
    "net.add(Layer(2, 3, activator))\n",
    "net.add(Layer(3, 1, activator))\n",
    "\n",
    "#x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]])\n",
    "#y_train = np.array([[[0]], [[1]], [[1]], [[0]]])\n",
    "#net.fit(x_train, y_train, epochs=1000, learn_rate=0.1)\n",
    "\n",
    "# training data\n",
    "x_train = [np.array([[0,0]]), np.array([[0,1]]), np.array([[1,0]]), np.array([[1,1]])]\n",
    "y_train = [np.array([[0]]), np.array([[1]]), np.array([[1]]), np.array([[0]])]\n",
    "#net.fit(x_train, y_train, epochs=1000, learn_rate=0.1)\n",
    "\n",
    "epochs = 1000\n",
    "batch_size = 1\n",
    "for i in range(epochs):\n",
    "    x_b, y_b = batchtize(x_train, y_train, batch_size)\n",
    "    for i in range(len(x_b)):\n",
    "        print(f\"x_b[i]: {x_b}\")\n",
    "        net.fit([x_train[i]], [y_train[i]], epochs=1, learn_rate=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a86314-af24-4ed0-b135-2001386681c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c6386a-1c5c-48e9-b09c-5de4681804ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44126165-61c1-4005-9f5b-572215e7bc47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3789fb4-a155-41a2-a597-5fc7eafb791e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
